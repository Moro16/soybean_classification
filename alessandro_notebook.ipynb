{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a73bdd-c14f-48cc-a6f4-cdd061fddc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"warcoder/soyabean-seeds\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f3ab27-7850-41e5-bc97-a7a46a6f660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"raw_data/Soybean Seeds\"\n",
    "train_path = \"raw_data/train\"\n",
    "val_path = \"raw_data/val\"\n",
    "test_path = \"raw_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa86aed-1378-47f2-a802-12c2f6c57297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9c6cbd-8054-4e14-8ef9-174e4caaca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(val_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe61a857-e6da-4243-ad87-a81ee9538e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_ratio = 0.7\n",
    "# val_ratio = .15\n",
    "\n",
    "# for class_folder in os.listdir(dataset_path):\n",
    "#     class_path = os.path.join(dataset_path, class_folder)\n",
    "#     images = os.listdir(class_path)\n",
    "#     random.shuffle(images)\n",
    "\n",
    "#     # Calculate the split index\n",
    "#     split_index = int(train_ratio * len(images))\n",
    "#     val_len = int(len(images) * (1-val_ratio))\n",
    "\n",
    "#     # Split the images into training and testing sets\n",
    "#     train_images = images[:split_index]\n",
    "#     val_images = images[split_index:val_len]\n",
    "#     test_images = images[val_len:]\n",
    "\n",
    "#     # Create class folders in the training and testing directories\n",
    "#     train_class_path = os.path.join(train_path, class_folder)\n",
    "#     val_class_path = os.path.join(val_path, class_folder)\n",
    "#     test_class_path = os.path.join(test_path, class_folder)\n",
    "#     os.makedirs(train_class_path, exist_ok=True)\n",
    "#     os.makedirs(val_class_path, exist_ok=True)\n",
    "#     os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "#     # Move images to the respective class folders in training and testing\n",
    "#     for image in train_images:\n",
    "#         src = os.path.join(class_path, image)\n",
    "#         dest = os.path.join(train_class_path, image)\n",
    "#         shutil.copy(src, dest)\n",
    "\n",
    "#     for image in val_images:\n",
    "#         src = os.path.join(class_path, image)\n",
    "#         dest = os.path.join(val_class_path, image)\n",
    "#         shutil.copy(src, dest)\n",
    "\n",
    "#     for image in test_images:\n",
    "#         src = os.path.join(class_path, image)\n",
    "#         dest = os.path.join(test_class_path, image)\n",
    "#         shutil.copy(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88a4948-3ec5-47e4-865d-4a628b3662fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:06:00.843931: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 19:06:00.843976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 19:06:00.845610: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 19:06:00.853367: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 19:06:01.959125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-28 19:06:02.804624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:02.842703: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:02.842761: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, save_model\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Flatten,Conv2D,Activation,Dropout,MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8e41e0-e2f0-4af8-bbd7-3a049bd49e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = load_img(f'{train_path}/Broken soybeans/3.jpg')\n",
    "# x = img_to_array(img)/255\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c586fb0d-eac8-4241-b3c0-948425c1fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a69776-a113-4e75-846a-99958a9bdab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"raw_data/train\"\n",
    "val_dir = \"raw_data/test\"\n",
    "test_dir = \"raw_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449e5385-0f1e-4c55-ba56-0da0f3afac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=[\"path\", \"class\"])\n",
    "val_df = pd.DataFrame(columns=[\"path\", \"class\"])\n",
    "test_df = pd.DataFrame(columns=[\"path\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5678bc-9cc8-4e45-bba9-20df164b605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_name in os.listdir(train_dir):\n",
    "#   class_dir = os.path.join(train_dir, class_name)\n",
    "#   for image_name in os.listdir(class_dir):\n",
    "#     image_path = os.path.join(class_dir, image_name)\n",
    "#     train_df.loc[len(train_df.index)] = [image_path, class_name]\n",
    "\n",
    "# for class_name in os.listdir(test_dir):\n",
    "#   class_dir = os.path.join(test_dir, class_name)\n",
    "#   for image_name in os.listdir(class_dir):\n",
    "#     image_path = os.path.join(class_dir, image_name)\n",
    "#     test_df.loc[len(test_df.index)] = [image_path, class_name]\n",
    "\n",
    "# for class_name in os.listdir(val_dir):\n",
    "#   class_dir = os.path.join(val_dir, class_name)\n",
    "#   for image_name in os.listdir(class_dir):\n",
    "#     image_path = os.path.join(class_dir, image_name)\n",
    "#     val_df.loc[len(val_df.index)] = [image_path, class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df8bcb3-76c8-4fa0-ab0a-973d267272d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_df.info(), val_df.info(), test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c0126a-2dab-4b14-9252-b3932dfef294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4fcc849-b8e7-443f-af44-63b9ecc83994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].set_title('loss')\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax[1].set_title('accuracy')\n",
    "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train acc\")\n",
    "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation acc\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff5bdc4-d3e3-4a29-9caa-b43b53a3e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_compare_history(history, name_history, history_1, name_history_1):\n",
    "\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "#     ax[0].set_title('loss')\n",
    "\n",
    "#     ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss \" + name_history)\n",
    "#     ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss \" + name_history)\n",
    "\n",
    "#     ax[0].plot(history_1.epoch, history_1.history[\"loss\"], label=\"Train loss \" + name_history_1)\n",
    "#     ax[0].plot(history_1.epoch, history_1.history[\"val_loss\"], label=\"Validation loss \" + name_history_1)\n",
    "\n",
    "#     ax[1].set_title('Accuracy')\n",
    "\n",
    "#     ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train Accuracy \" + name_history)\n",
    "#     ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history)\n",
    "\n",
    "#     ax[1].plot(history_1.epoch, history_1.history[\"accuracy\"], label=\"Train Accuracy \" + name_history_1)\n",
    "#     ax[1].plot(history_1.epoch, history_1.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history_1)\n",
    "\n",
    "#     ax[0].legend()\n",
    "#     ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4da00-6cdb-46ca-b744-ffa966459ad7",
   "metadata": {},
   "source": [
    "# PRIMEIRO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7916e01f-7833-4291-976b-40b223a51839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(layers.Rescaling(1/255, input_shape=(227,227,3)))\n",
    "\n",
    "# model.add(layers.RandomFlip(mode='horizontal_and_vertical'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(5,5), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(3,3), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(2,2), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(2,2), padding='same'))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(layers.Dense(96, activation='relu'))\n",
    "# model.add(layers.Dropout(.5))\n",
    "\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dropout(.5))\n",
    "\n",
    "# model.add(layers.Dense(5,activation=\"softmax\"))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b95585-b8fa-488a-8fdf-be4668386ba1",
   "metadata": {},
   "source": [
    "# MODELO MELHORADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d6c60c-ebe3-453d-8206-98f240137bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 20:41:14.663318: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 20:41:14.663395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 20:41:14.663429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 20:41:14.802498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 20:41:14.802565: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 20:41:14.802572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-26 20:41:14.802644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-26 20:41:14.802677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5105 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " random_flip_1 (RandomFlip)  (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 43, 43, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 43, 43, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 43, 43, 64)        32832     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 15, 15, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 15, 15, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 15, 15, 128)       131200    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 8, 8, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 4, 4, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 4, 4, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               524416    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 996549 (3.80 MB)\n",
      "Trainable params: 995589 (3.80 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Rescaling(1/255, input_shape=(128,128,3)))\n",
    "\n",
    "model.add(layers.RandomFlip(mode='horizontal'))\n",
    "model.add(layers.RandomFlip(mode='vertical'))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(3,3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(3,3), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(4,4), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(.4))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(.4))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(.4))\n",
    "\n",
    "model.add(layers.Dense(5,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ab3ae-b7da-4031-96ec-eb91b6f8719c",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ae9d32-c9d5-4053-b74c-59956dfaebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(layers.Rescaling(1/255, input_shape=(227,227,3)))\n",
    "\n",
    "# model.add(layers.RandomFlip(mode='horizontal_and_vertical'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "# model.add(layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(layers.Dense(256, activation='relu'))\n",
    "\n",
    "# model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(layers.Dense(5,activation=\"softmax\"))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c02eff52-d476-4fe2-bb67-ef44e9abdcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Dense,Flatten,Conv2D,Activation,Dropout,MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca81e9f7-98c5-4e5a-9ca5-da27cf5ae6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(input_shape=(227,227,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name='vgg16'))\n",
    "# model.add(Flatten(name='flatten'))\n",
    "# model.add(Dense(256, activation='relu', name='fc1'))\n",
    "# model.add(Dense(128, activation='relu', name='fc2'))\n",
    "# model.add(Dense(5, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c60887-6f47-4a6e-b83c-ecfe5e4f7dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = optimizers.SGD(learning_rate=1e-6, momentum=0.9)\n",
    "# model.compile(loss=\"categorical_crossentropy\",\n",
    "#               optimizer=opt,\n",
    "#               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a427731-441a-4394-aabf-ae312f718cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:06:18.301878: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:18.301958: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:18.301992: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:18.481991: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:18.482049: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:18.482058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-28 19:06:18.482128: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-28 19:06:18.482168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5592 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m adam \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m              optimizer\u001b[38;5;241m=\u001b[39madam,\n\u001b[1;32m      4\u001b[0m              metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "adam = optimizers.Adam(learning_rate = .001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=adam,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c739cd-7971-470c-a15a-94f1c8fe40be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3856 files belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:06:53.249564: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 830 files belonging to 5 classes.\n",
      "Found 830 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_ds = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['Broken soybeans', 'Immature soybeans', 'Intact soybeans', 'Skin-damaged soybeans', 'Spotted soybeans'],\n",
    "    image_size=(128,128),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['Broken soybeans', 'Immature soybeans', 'Intact soybeans', 'Skin-damaged soybeans', 'Spotted soybeans'],\n",
    "    image_size=(128,128),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = 'categorical',\n",
    "    class_names = ['Broken soybeans', 'Immature soybeans', 'Intact soybeans', 'Skin-damaged soybeans', 'Spotted soybeans'],\n",
    "    image_size=(128,128),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b9d5a75-683c-40da-a8bd-a101214c8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = train_ds.class_names\n",
    "# print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20cd2244-40be-41d3-b82a-883289e78e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"model_1.keras\"\n",
    "\n",
    "modelCheckpoint = callbacks.ModelCheckpoint(MODEL,\n",
    "                                            monitor=\"val_accuracy\",\n",
    "                                            verbose=0,\n",
    "                                            save_best_only=True)\n",
    "\n",
    "LRreducer = callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",\n",
    "                                        factor=0.1,\n",
    "                                        patience=5,\n",
    "                                        verbose=1,\n",
    "                                        min_lr=0)\n",
    "\n",
    "EarlyStopper = callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                       patience=15,\n",
    "                                       verbose=0,\n",
    "                                       restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d262cf2f-3a2a-41d0-8e6d-d442bc30da8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 11:08:49.792328: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-23 11:08:49.927783: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-23 11:08:49.927830: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13845 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-23 11:08:49.929256: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-23 11:08:49.929304: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13845 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-23 11:08:49.931749: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-23 11:08:49.931795: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13845 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/61 [==>...........................] - ETA: 6s - loss: 2.2504 - accuracy: 0.2857"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/soybean_classification/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=900,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[modelCheckpoint, LRreducer, EarlyStopper]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "939e5ba9-0124-4de9-908f-d6d679ab5219",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_history(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2eb5e47-611a-42b7-b80c-bddc6eb7c1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 20:41:28.593551: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-11-26 20:41:28.686638: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 26ms/step - loss: 1.6114 - accuracy: 0.1916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.611382007598877, 0.19156625866889954]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b0c6fb-7b24-4be5-bb0e-6921207eeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model_val_loss:0.3586-val_accuracy:0.9048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70670126-07e7-4e57-8ccf-e627c75ebf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_90 = load_model(\"model_val_loss:0.3586-val_accuracy:0.9048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d1fd37a-e163-4a28-b7cd-cedcbfabc091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:06:57.801157: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-11-28 19:06:57.963274: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 54ms/step - loss: 0.3586 - accuracy: 0.9048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3586433529853821, 0.9048192501068115]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_90.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aafc54b2-227a-473f-8aa7-d54534657767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 18ms/step - loss: 0.3586 - accuracy: 0.9048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3586433231830597, 0.9048192501068115]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_90.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab7586f-4a2c-46e2-9262-7b665ed4773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.497973  , 0.02437695, 0.07462369, 0.2988353 , 0.10419106]],\n",
       "       dtype=float32),\n",
       " 'Broken soybeans',\n",
       " 49.8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = load_img(f'{test_path}/Broken soybeans/63.jpg', target_size=(128,128))\n",
    "img_array = img_to_array(img)\n",
    "x = np.expand_dims(img_array, axis=0)\n",
    "pred = model_90.predict(x)\n",
    "numero_classe = list(pred[0]).index(pred.max())\n",
    "classe_lista = ['Broken soybeans', 'Immature soybeans', 'Intact soybeans', 'Skin-damaged soybeans', 'Spotted soybeans']\n",
    "pred, classe_lista[numero_classe],round(pred.max()*100, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soybean_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
